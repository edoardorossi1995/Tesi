{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3vWTd_Ih2p9d"
      ],
      "authorship_tag": "ABX9TyMCWl7UgQrTKpPt0sADGsbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edoardorossi1995/Tesi/blob/main/Average_Reward_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "\n",
        "Algoritmo Q-Learningc (off policy)\n",
        "\n",
        "Task non episodico (continuativo)\n",
        "\n",
        "Framework: Average Reward\n",
        "\n",
        "Approssimatore funzionale: Rete Neurale Convessa *(vedi cit.)*"
      ],
      "metadata": {
        "id": "3vWTd_Ih2p9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Drive Setting\n"
      ],
      "metadata": {
        "id": "q46girwt63TH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jajCZgNk6wXT",
        "outputId": "691094dc-0a78-423e-9e38-b149eb7f60c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/Tesi\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB == True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  %cd /content/gdrive/MyDrive/Tesi\n",
        "  #!ls\n",
        "\n",
        "import warnings\n",
        "import pickle\n",
        "import sys\n",
        "if IN_COLAB == True:\n",
        "  sys.path.insert(0, '/content/gdrive/MyDrive/Tesi/functions')\n",
        "  sys.path.insert(0, '/content/gdrive/MyDrive/Tesi')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Lambda\n",
        "from keras.layers import Activation\n",
        "from keras.activations import exponential, relu\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "from pkl import store_data, load_data\n",
        "from functions.compressor import compress\n",
        "\n",
        "import random\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Anagrafica e CTA"
      ],
      "metadata": {
        "id": "nQw2LmXT7FOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "CTA30_def = ['/content/gdrive/MyDrive/Tesi/CTA30/DF_clean/clean_CTA_30_def.csv']\n",
        "df_def = pd.read_csv(CTA30_def[0])\n",
        "\n",
        "CTA30_18 = ['/content/gdrive/MyDrive/Tesi/CTA30/DF_clean/clean_CTA_30_2018.csv']\n",
        "df = pd.read_csv(CTA30_18[0])\n",
        "df['VLV_RAF_FDBK_00'] = df['VLV_RAF_FDBK_00'] / 100\n",
        "df['VLV_SUR_FDBK_00'] = df['VLV_SUR_FDBK_00'] / 100\n",
        "scaler = MinMaxScaler()\n",
        "act_f_m = df.loc[:,'F_M_FDBK_00']\n",
        "a_f_m_reshaped = np.array(act_f_m).reshape(-1, 1)\n",
        "a_f_m_scaled = scaler.fit_transform(a_f_m_reshaped)\n",
        "\n",
        "pca_df = compress(df)"
      ],
      "metadata": {
        "id": "v7Tx5Dbc6y7G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Anagrafica shape:\",df_def.shape,\"\\nDataframe shape:\", df.shape,\"\\nPca Dataframe shape:\", pca_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QMdpC4_8QV-",
        "outputId": "b6d97b08-95a5-4e5e-cc04-df265a8e97be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anagrafica shape: (76, 2) \n",
            "Dataframe shape: (141811, 76) \n",
            "Pca Dataframe shape: (141811, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_def"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sQRpOWyFHXi6",
        "outputId": "311cf7c1-8de2-44d4-eaaf-2dc5677f13b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           pointColName                            pointName\n",
              "0   AUTOMAN_SERR_CMD_00  Automatico Manuale Serrande Comando\n",
              "1    AUTOMAN_VLV_CMD_00   Automatico Manuale Valvole Comando\n",
              "2      AUTOMAN_M_CMD_00   Automatico Manuale Mandata Comando\n",
              "3      AUTOMAN_R_CMD_00   Automatico Manuale Ripresa Comando\n",
              "4         LOCREM_CMD_00      Selettore Locale Remoto Comando\n",
              "..                  ...                                  ...\n",
              "71            VOLT_R_00                     Tensione Ripresa\n",
              "72            T_M_SP_00    Temperatura Aria Mandata Setpoint\n",
              "73            T_R_SP_00    Temperatura Aria Ritorno Setpoint\n",
              "74             T_AMB_04      Temperatura Aria Ambiente Media\n",
              "75        AUTOMAN_ST_00             Automatico Manuale Stato\n",
              "\n",
              "[76 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-580b0c42-61f9-49a9-b2df-510cfcaeef92\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pointColName</th>\n",
              "      <th>pointName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AUTOMAN_SERR_CMD_00</td>\n",
              "      <td>Automatico Manuale Serrande Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AUTOMAN_VLV_CMD_00</td>\n",
              "      <td>Automatico Manuale Valvole Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AUTOMAN_M_CMD_00</td>\n",
              "      <td>Automatico Manuale Mandata Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AUTOMAN_R_CMD_00</td>\n",
              "      <td>Automatico Manuale Ripresa Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LOCREM_CMD_00</td>\n",
              "      <td>Selettore Locale Remoto Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>VOLT_R_00</td>\n",
              "      <td>Tensione Ripresa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>T_M_SP_00</td>\n",
              "      <td>Temperatura Aria Mandata Setpoint</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>T_R_SP_00</td>\n",
              "      <td>Temperatura Aria Ritorno Setpoint</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>T_AMB_04</td>\n",
              "      <td>Temperatura Aria Ambiente Media</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>AUTOMAN_ST_00</td>\n",
              "      <td>Automatico Manuale Stato</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-580b0c42-61f9-49a9-b2df-510cfcaeef92')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-580b0c42-61f9-49a9-b2df-510cfcaeef92 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-580b0c42-61f9-49a9-b2df-510cfcaeef92');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rete Neurale\n"
      ],
      "metadata": {
        "id": "SmYeYm2EJYFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Acts\n",
        "\n",
        "Efficient reinforcement learning via difference of\n",
        "log-sum-exp neural networks\n",
        "\n",
        "(Sven Bru ̈ggemann and Corrado Possieri)"
      ],
      "metadata": {
        "id": "uXFYBZPCM5ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpAct(Layer):\n",
        "    def __init__(self, inner_dim, **kwargs):\n",
        "        super(ExpAct, self).__init__(**kwargs)\n",
        "        self.T = 0.1\n",
        "        self.a = self.add_weight(name=\"a\", shape=(input_dim, inner_dim), initializer=\"glorot_normal\")\n",
        "        self.b = self.add_weight(name=\"b\", shape=(inner_dim,), initializer=\"zeros\")\n",
        "        self.inner_dim = inner_dim\n",
        "        \n",
        "    def call(self, x):\n",
        "        z = x / self.T\n",
        "        PSI = self.psi(x, self.a, self.b)\n",
        "        p = (K.dot(self.a, z) + self.b) / self.T - PSI\n",
        "        return p\n",
        "\n",
        "    def psi(self, x, a, b):\n",
        "        z = x / self.T\n",
        "        PSI = K.dot(a, z) / self.T + b\n",
        "        PSI = K.exp(PSI - K.max(PSI))  # usa K.exp e K.max\n",
        "        PSI /= K.sum(PSI, axis=-1, keepdims=True)\n",
        "        PSI *= self.inner_dim\n",
        "        return PSI\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.inner_dim)\n",
        "\n",
        "class LogAct(Layer):\n",
        "    def __init__(self, inner_dim, **kwargs):\n",
        "        super(LogAct, self).__init__(**kwargs)\n",
        "        self.T = 0.1\n",
        "        self.a = self.add_weight(name=\"a\", shape=(input_dim, inner_dim), initializer=\"glorot_normal\")\n",
        "        self.b = self.add_weight(name=\"b\", shape=(inner_dim,), initializer=\"zeros\")\n",
        "        self.inner_dim = inner_dim\n",
        "\n",
        "    def call(self, x):\n",
        "        z = x / self.T\n",
        "        PSI = self.psi(x, self.a, self.b)\n",
        "        p = self.T * (PSI + K.log(x))\n",
        "        return p\n",
        "\n",
        "    def psi(self, x, a, b):\n",
        "        z = x / self.T    \n",
        "        z = tf.transpose(tf.expand_dims(x, axis=1), perm=[0, 2, 1])\n",
        "        PSI = K.dot(a,z) / self.T + b\n",
        "        PSI = K.exp(PSI - K.max(PSI))\n",
        "        PSI /= K.sum(PSI, axis=-1, keepdims=True)\n",
        "        PSI *= self.inner_dim\n",
        "        return PSI\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.inner_dim)"
      ],
      "metadata": {
        "id": "HlMQzEbT_6XX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NN Creation\n",
        "\n",
        "Log-sum-exp neural networks and posynomial\n",
        "models for convex and log-log-convex data \n",
        "\n",
        "(Calafiore, Gaubert, Possieri)"
      ],
      "metadata": {
        "id": "Gb0BUVXXM_-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = load_data('*.pkl')\n",
        "action_vector_dimension = 2\n",
        "\n",
        "# Architettura rete neurale\n",
        "input_dim = n['n_components'] + action_vector_dimension\n",
        "inner_dim_1 = 32\n",
        "output_dim = 1\n",
        "\n",
        "print(\"input dim: \", input_dim)\n",
        "print(\"inner layer dim: \", inner_dim_1)\n",
        "print(\"output dim: \", output_dim)\n",
        "\n",
        "# Costruzione strati\n",
        "rete_neurale = Sequential()\n",
        "\n",
        "# Primo strato\n",
        "rete_neurale.add(Dense(inner_dim_1, input_shape=(input_dim,), activation=ExpAct(inner_dim=inner_dim_1)))\n",
        "\n",
        "# Secondo strato\n",
        "rete_neurale.add(Dense(output_dim, activation=LogAct(inner_dim=output_dim)))\n",
        "\n",
        "\n",
        "rete_neurale.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])\n",
        "\n",
        "# Definisco funzione obiettivo per la massimizzazione della predizione \n",
        "\n",
        "def objective(x):\n",
        "    pca_s_prime = pca_df.loc[s_prime,:]\n",
        "    nn_input_prime = np.concatenate((pca_s_prime.values.reshape(7,1), x[0].reshape(1,1), x[1].reshape(1,1)), axis=0)\n",
        "    nn_input_prime = np.reshape(nn_input_prime, (1,input_dim))\n",
        "    funct = rete_neurale.predict(nn_input_prime, verbose=0)[0][0]\n",
        "    return -funct"
      ],
      "metadata": {
        "id": "nSB60YSmJZ_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a882c556-5147-445d-d7f5-c1c2d1d00d13"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input dim:  9\n",
            "inner layer dim:  32\n",
            "output dim:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(df.loc[5000:6000,'PW_ATT_M_00'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OXGSdId9Tfr",
        "outputId": "7d5fdc0b-136d-48af-89af-5b4e39f4995f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000    19.8775\n",
              "5001    19.7422\n",
              "5002    19.7584\n",
              "5003    19.7964\n",
              "5004    19.8194\n",
              "         ...   \n",
              "5996    20.0913\n",
              "5997    20.0210\n",
              "5998    20.0126\n",
              "5999    19.9825\n",
              "6000    20.0735\n",
              "Name: PW_ATT_M_00, Length: 1001, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Agent"
      ],
      "metadata": {
        "id": "_KF3EgbAI_y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensione problema e inizializzazione stato iniziale\n",
        "[S, feat] = df.shape\n",
        "current_state = 0 # np.random.randint(0,S-1)\n",
        "\n",
        "# Stato corrente\n",
        "s = current_state\n",
        "\n",
        "min_f = np.min(a_f_m_scaled)\n",
        "max_f = np.max(a_f_m_scaled)\n",
        "\n",
        "# Parametri di apprendimento (learning rate, discount factor, average rate)\n",
        "alpha = 0.5 # non in uso\n",
        "gamma = 1\n",
        "beta = 1\n",
        "\n",
        "# Media mobile esponenziale (avr = avr + beta * (r - avr)) \n",
        "# [scelta perché computazionalmente più leggera di una media aritmetica]\n",
        "average_reward = -np.random.randint(np.min(df.loc[:,'PW_ATT_M_00']),np.max(df.loc[:,'PW_ATT_M_00']))\n",
        "print(\"average_reward start: \", average_reward,\"\\n\")\n",
        "\n",
        "# Setting Episodi\n",
        "total_episodes = 50000\n",
        "current_episode = 0\n",
        "\n",
        "# Ripeti finché non raggiungi la fine del processo\n",
        "for current_episode in range(total_episodes):\n",
        "    \n",
        "    # Stato successivo e ottieni l'azione A e la ricompensa R\n",
        "    s_prime = s+1\n",
        "    if s_prime > S-1:\n",
        "      print(\"s =\", s)\n",
        "      break\n",
        "\n",
        "    a1 = df.loc[s,'F_M_FDBK_00']\n",
        "    a2 = df.loc[s,'VLV_RAF_FDBK_00']\n",
        "    a = (a1,a2)\n",
        "    r = -df.loc[s,'PW_ATT_M_00']\n",
        "    average_reward = average_reward + beta * (r - average_reward)\n",
        "\n",
        "    # Condizioni iniziali (random) della minimizzazione della funzione obiettivo \n",
        "    # per la predizione di q'\n",
        "    data_f = a_f_m_scaled\n",
        "    a0_f = data_f[np.random.randint(len(data_f))]\n",
        "    a0_v = np.random.rand()\n",
        "    x0 = [a0_f, a0_v]\n",
        "\n",
        "    # Dominio della minimizzazione \n",
        "    bounds = [(min_f, max_f), (0,1)]\n",
        "\n",
        "    # Risultato della minimizzazione\n",
        "    objective_minimized = minimize(objective, x0, method = 'Nelder-Mead', bounds = bounds)\n",
        "\n",
        "    # Compongo input della predizione di q'\n",
        "    a1_prime_input = objective_minimized.x[0].reshape(1,1)\n",
        "    a2_prime_input = objective_minimized.x[1].reshape(1,1)\n",
        "    s_prime_input = pca_df.loc[s_prime,:].values.reshape(7,1)\n",
        "    nn_input_prime = np.concatenate((s_prime_input, a1_prime_input, a2_prime_input),axis = 0)\n",
        "    nn_input_prime = np.reshape(nn_input_prime, (1,input_dim))\n",
        "\n",
        "    # Predizione di q'\n",
        "    q_prime = rete_neurale.predict(nn_input_prime, verbose = 0)[0]\n",
        "    q_prime = q_prime[0]\n",
        "    # Calcola la media delle ricompense future attese per lo stato-azione corrente\n",
        "    # q = avr + gamma*max(q')\n",
        "    #print(\"q' = \", q_prime)\n",
        "    q = (average_reward + gamma * (q_prime))\n",
        "    #print(\"q = \", q)\n",
        "\n",
        "    # Calcola il valore target per il training della rete neurale\n",
        "    target = q\n",
        "    target = np.array([target])\n",
        "\n",
        "    # Aggiorna la rete neurale\n",
        "    pca_s = pca_df.loc[s,:]\n",
        "    nn_input = np.concatenate((pca_s.values, np.array([a[0],a[1]])))\n",
        "    nn_input = np.reshape(nn_input, (1,input_dim))    \n",
        "    rete_neurale.fit(nn_input, target, epochs=1, verbose=0)\n",
        "\n",
        "    # Aggiorna lo stato corrente\n",
        "    s = s_prime\n",
        "\n",
        "    # Plot statistiche ogni 100 episodi\n",
        "    print(\"current_episode = \", current_episode)\n",
        "    n_print = 1\n",
        "    if current_episode%n_print == 0:\n",
        "      print(\"Episodio \",current_episode,\"/\",total_episodes)\n",
        "      print(\"target: \", target, \"\\nreward: \",r,\"\\naverage_reward: \",np.mean(average_reward))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Restituisci la rete neurale ottenuta\n",
        "#return rete_neurale"
      ],
      "metadata": {
        "id": "_Dh_OuPqAJn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eJajUUX-Nw3f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
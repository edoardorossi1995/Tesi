{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3vWTd_Ih2p9d"
      ],
      "authorship_tag": "ABX9TyOo68ySQTgTUPyHkjrl2K7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edoardorossi1995/Tesi/blob/main/Average_Reward_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pseudocodice \n",
        "###Algoritmo di tipo Q-learning, off policy, a task non episodico, che approssima la funzione valore con una rete neurale convessa, con un framework di tipo average reward."
      ],
      "metadata": {
        "id": "3vWTd_Ih2p9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Drive Setting\n"
      ],
      "metadata": {
        "id": "q46girwt63TH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jajCZgNk6wXT",
        "outputId": "1fe865fe-9fc1-4e1b-9e99-4cd66b14ccc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Tesi\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB == True:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  %cd /content/gdrive/MyDrive/Tesi\n",
        "  #!ls\n",
        "\n",
        "import warnings\n",
        "import pickle\n",
        "import sys\n",
        "if IN_COLAB == True:\n",
        "  sys.path.insert(0, '/content/gdrive/MyDrive/Tesi/functions')\n",
        "  sys.path.insert(0, '/content/gdrive/MyDrive/Tesi')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Lambda\n",
        "from keras.activations import exponential, relu\n",
        "from keras import backend as K\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "from pkl import store_data, load_data\n",
        "from functions.compressor import compress\n",
        "\n",
        "import random\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Anagrafica e CTA"
      ],
      "metadata": {
        "id": "nQw2LmXT7FOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "CTA30_def = ['/content/gdrive/MyDrive/Tesi/CTA30/DF_clean/clean_CTA_30_def.csv']\n",
        "df_def = pd.read_csv(CTA30_def[0])\n",
        "\n",
        "CTA30_18 = ['/content/gdrive/MyDrive/Tesi/CTA30/DF_clean/clean_CTA_30_2018.csv']\n",
        "df = pd.read_csv(CTA30_18[0])\n",
        "df['VLV_RAF_FDBK_00'] = df['VLV_RAF_FDBK_00'] / 100\n",
        "df['VLV_SUR_FDBK_00'] = df['VLV_SUR_FDBK_00'] / 100\n",
        "scaler = MinMaxScaler()\n",
        "act_f_m = df.loc[:,'F_M_FDBK_00']\n",
        "a_f_m_reshaped = np.array(act_f_m).reshape(-1, 1)\n",
        "a_f_m_scaled = scaler.fit_transform(a_f_m_reshaped)\n",
        "\n",
        "pca_df = compress(df)"
      ],
      "metadata": {
        "id": "v7Tx5Dbc6y7G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Anagrafica shape:\",df_def.shape,\"\\nDataframe shape:\", df.shape,\"\\nPca Dataframe shape:\", pca_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QMdpC4_8QV-",
        "outputId": "2b1dbf6d-85e1-4ad4-b262-ca074927d06a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anagrafica shape: (76, 2) \n",
            "Dataframe shape: (141811, 76) \n",
            "Pca Dataframe shape: (141811, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_def"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sQRpOWyFHXi6",
        "outputId": "df49a757-953e-4a3c-b1dd-bf9d9906be42"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           pointColName                            pointName\n",
              "0   AUTOMAN_SERR_CMD_00  Automatico Manuale Serrande Comando\n",
              "1    AUTOMAN_VLV_CMD_00   Automatico Manuale Valvole Comando\n",
              "2      AUTOMAN_M_CMD_00   Automatico Manuale Mandata Comando\n",
              "3      AUTOMAN_R_CMD_00   Automatico Manuale Ripresa Comando\n",
              "4         LOCREM_CMD_00      Selettore Locale Remoto Comando\n",
              "..                  ...                                  ...\n",
              "71            VOLT_R_00                     Tensione Ripresa\n",
              "72            T_M_SP_00    Temperatura Aria Mandata Setpoint\n",
              "73            T_R_SP_00    Temperatura Aria Ritorno Setpoint\n",
              "74             T_AMB_04      Temperatura Aria Ambiente Media\n",
              "75        AUTOMAN_ST_00             Automatico Manuale Stato\n",
              "\n",
              "[76 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb9cf81a-10c2-4181-89f9-f7224e80e7be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pointColName</th>\n",
              "      <th>pointName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AUTOMAN_SERR_CMD_00</td>\n",
              "      <td>Automatico Manuale Serrande Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AUTOMAN_VLV_CMD_00</td>\n",
              "      <td>Automatico Manuale Valvole Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AUTOMAN_M_CMD_00</td>\n",
              "      <td>Automatico Manuale Mandata Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AUTOMAN_R_CMD_00</td>\n",
              "      <td>Automatico Manuale Ripresa Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LOCREM_CMD_00</td>\n",
              "      <td>Selettore Locale Remoto Comando</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>VOLT_R_00</td>\n",
              "      <td>Tensione Ripresa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>T_M_SP_00</td>\n",
              "      <td>Temperatura Aria Mandata Setpoint</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>T_R_SP_00</td>\n",
              "      <td>Temperatura Aria Ritorno Setpoint</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>T_AMB_04</td>\n",
              "      <td>Temperatura Aria Ambiente Media</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>AUTOMAN_ST_00</td>\n",
              "      <td>Automatico Manuale Stato</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb9cf81a-10c2-4181-89f9-f7224e80e7be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bb9cf81a-10c2-4181-89f9-f7224e80e7be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bb9cf81a-10c2-4181-89f9-f7224e80e7be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rete Neurale\n",
        "\n",
        "Log-sum-exp neural networks and posynomial\n",
        "models for convex and log-log-convex data \n",
        "\n",
        "(Calafiore, Gaubert, Possieri)"
      ],
      "metadata": {
        "id": "SmYeYm2EJYFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = load_data('*.pkl')\n",
        "action_vector_dimension = 2\n",
        "\n",
        "# Funzione di attivazione logatirmica (ultimo strato)\n",
        "def log_act(x):\n",
        "  return K.log(x+10)\n",
        "\n",
        "\n",
        "# Architettura rete neurale\n",
        "input_dim = n['n_components']+action_vector_dimension\n",
        "inner_dim_1 = 32\n",
        "output_dim = 1\n",
        "\n",
        "print(\"input dim: \", input_dim)\n",
        "print(\"inner layer dim: \", inner_dim_1)\n",
        "print(\"output dim: \", output_dim)\n",
        "\n",
        "# Costruzione strati\n",
        "rete_neurale = Sequential()\n",
        "rete_neurale.add(Dense(units = inner_dim_1, activation = exponential, input_dim = input_dim))\n",
        "rete_neurale.add(Dense(units = output_dim, activation=log_act)) \n",
        "\n",
        "rete_neurale.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])\n",
        "\n",
        "# Definisco funzione obiettivo per la massimizzazione della predizione \n",
        "\n",
        "def objective(x):\n",
        "    pca_s_prime = pca_df.loc[s_prime,:]\n",
        "    nn_input_prime = np.concatenate((pca_s_prime.values.reshape(7,1), x[0].reshape(1,1), x[1].reshape(1,1)), axis=0)\n",
        "    nn_input_prime = np.reshape(nn_input_prime, (1,input_dim))\n",
        "    funct = rete_neurale.predict(nn_input_prime, verbose=0)[0][0]\n",
        "    return -funct"
      ],
      "metadata": {
        "id": "nSB60YSmJZ_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fbadb7b-241d-4ba0-f97b-1480b78bb34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input dim:  9\n",
            "inner layer dim:  32\n",
            "output dim:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(df.loc[5000:6000,'PW_ATT_M_00'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OXGSdId9Tfr",
        "outputId": "0a27d30c-ed28-4e4e-ca30-abb4870bcd33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000    19.8775\n",
              "5001    19.7422\n",
              "5002    19.7584\n",
              "5003    19.7964\n",
              "5004    19.8194\n",
              "         ...   \n",
              "5996    20.0913\n",
              "5997    20.0210\n",
              "5998    20.0126\n",
              "5999    19.9825\n",
              "6000    20.0735\n",
              "Name: PW_ATT_M_00, Length: 1001, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Agent"
      ],
      "metadata": {
        "id": "_KF3EgbAI_y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensione problema e inizializzazione stato iniziale\n",
        "[S, feat] = df.shape\n",
        "current_state = 0 # np.random.randint(0,S-1)\n",
        "\n",
        "# Stato corrente\n",
        "s = current_state\n",
        "\n",
        "min_f = np.min(a_f_m_scaled)\n",
        "max_f = np.max(a_f_m_scaled)\n",
        "\n",
        "# Parametri di apprendimento (learning rate, discount factor, average rate)\n",
        "alpha = 0.5 # non in uso\n",
        "gamma = 1\n",
        "beta = 1\n",
        "\n",
        "# Media mobile esponenziale (avr = avr + beta * (r - avr)) \n",
        "# [scelta perché computazionalmente più leggera di una media aritmetica]\n",
        "average_reward = -np.random.randint(np.min(df.loc[:,'PW_ATT_M_00']),np.max(df.loc[:,'PW_ATT_M_00']))\n",
        "print(\"average_reward start: \", average_reward,\"\\n\")\n",
        "\n",
        "# Setting Episodi\n",
        "total_episodes = 50000\n",
        "current_episode = 0\n",
        "\n",
        "# Ripeti finché non raggiungi la fine del processo\n",
        "for current_episode in range(total_episodes):\n",
        "    \n",
        "    # Stato successivo e ottieni l'azione A e la ricompensa R\n",
        "    s_prime = s+1\n",
        "    if s_prime > S-1:\n",
        "      print(\"s =\", s)\n",
        "      break\n",
        "\n",
        "    a1 = df.loc[s,'F_M_FDBK_00']\n",
        "    a2 = df.loc[s,'VLV_RAF_FDBK_00']\n",
        "    a = (a1,a2)\n",
        "    r = -df.loc[s,'PW_ATT_M_00']\n",
        "    average_reward = average_reward + beta * (r - average_reward)\n",
        "\n",
        "    # Condizioni iniziali (random) della minimizzazione della funzione obiettivo \n",
        "    # per la predizione di q'\n",
        "    data_f = a_f_m_scaled\n",
        "    a0_f = data_f[np.random.randint(len(data_f))]\n",
        "    a0_v = np.random.rand()\n",
        "    x0 = [a0_f, a0_v]\n",
        "\n",
        "    # Dominio della minimizzazione \n",
        "    bounds = [(min_f, max_f), (0,1)]\n",
        "\n",
        "    # Risultato della minimizzazione\n",
        "    objective_minimized = minimize(objective, x0, method = 'Nelder-Mead', bounds = bounds)\n",
        "\n",
        "    # Compongo input della predizione di q'\n",
        "    a1_prime_input = objective_minimized.x[0].reshape(1,1)\n",
        "    a2_prime_input = objective_minimized.x[1].reshape(1,1)\n",
        "    s_prime_input = pca_df.loc[s_prime,:].values.reshape(7,1)\n",
        "    nn_input_prime = np.concatenate((s_prime_input, a1_prime_input, a2_prime_input),axis = 0)\n",
        "    nn_input_prime = np.reshape(nn_input_prime, (1,input_dim))\n",
        "\n",
        "    # Predizione di q'\n",
        "    q_prime = rete_neurale.predict(nn_input_prime, verbose = 0)[0]\n",
        "    q_prime = q_prime[0]\n",
        "    # Calcola la media delle ricompense future attese per lo stato-azione corrente\n",
        "    # q = avr + gamma*max(q')\n",
        "    #print(\"q' = \", q_prime)\n",
        "    q = (average_reward + gamma * (q_prime))\n",
        "    #print(\"q = \", q)\n",
        "\n",
        "    # Calcola il valore target per il training della rete neurale\n",
        "    target = q\n",
        "    target = np.array([target])\n",
        "\n",
        "    # Aggiorna la rete neurale\n",
        "    pca_s = pca_df.loc[s,:]\n",
        "    nn_input = np.concatenate((pca_s.values, np.array([a[0],a[1]])))\n",
        "    nn_input = np.reshape(nn_input, (1,input_dim))    \n",
        "    rete_neurale.fit(nn_input, target, epochs=1, verbose=0)\n",
        "\n",
        "    # Aggiorna lo stato corrente\n",
        "    s = s_prime\n",
        "\n",
        "    # Plot statistiche ogni 100 episodi\n",
        "    print(\"current_episode = \", current_episode)\n",
        "    n_print = 1\n",
        "    if current_episode%n_print == 0:\n",
        "      print(\"Episodio \",current_episode,\"/\",total_episodes)\n",
        "      print(\"target: \", target, \"\\nreward: \",r,\"\\naverage_reward: \",np.mean(average_reward))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Restituisci la rete neurale ottenuta\n",
        "#return rete_neurale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_Dh_OuPqAJn6",
        "outputId": "03618b11-d74a-41b6-97f1-ea8862bf87f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average_reward start:  -55 \n",
            "\n",
            "current_episode =  0\n",
            "Episodio  0 / 50000\n",
            "target:  [-25.13659167] \n",
            "reward:  -0.0 \n",
            "average_reward:  -27.5\n",
            "\n",
            "\n",
            "current_episode =  1\n",
            "Episodio  1 / 50000\n",
            "target:  [-11.39047813] \n",
            "reward:  -0.0 \n",
            "average_reward:  -13.75\n",
            "\n",
            "\n",
            "current_episode =  2\n",
            "Episodio  2 / 50000\n",
            "target:  [-4.51780367] \n",
            "reward:  -0.0 \n",
            "average_reward:  -6.875\n",
            "\n",
            "\n",
            "current_episode =  3\n",
            "Episodio  3 / 50000\n",
            "target:  [-1.08338881] \n",
            "reward:  -0.0 \n",
            "average_reward:  -3.4375\n",
            "\n",
            "\n",
            "current_episode =  4\n",
            "Episodio  4 / 50000\n",
            "target:  [0.63226318] \n",
            "reward:  -0.0 \n",
            "average_reward:  -1.71875\n",
            "\n",
            "\n",
            "current_episode =  5\n",
            "Episodio  5 / 50000\n",
            "target:  [1.48847651] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.859375\n",
            "\n",
            "\n",
            "current_episode =  6\n",
            "Episodio  6 / 50000\n",
            "target:  [1.9150362] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.4296875\n",
            "\n",
            "\n",
            "current_episode =  7\n",
            "Episodio  7 / 50000\n",
            "target:  [2.12643623] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.21484375\n",
            "\n",
            "\n",
            "current_episode =  8\n",
            "Episodio  8 / 50000\n",
            "target:  [2.23105073] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.107421875\n",
            "\n",
            "\n",
            "current_episode =  9\n",
            "Episodio  9 / 50000\n",
            "target:  [2.28164601] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.0537109375\n",
            "\n",
            "\n",
            "current_episode =  10\n",
            "Episodio  10 / 50000\n",
            "target:  [2.30524492] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.02685546875\n",
            "\n",
            "\n",
            "current_episode =  11\n",
            "Episodio  11 / 50000\n",
            "target:  [2.31550097] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.013427734375\n",
            "\n",
            "\n",
            "current_episode =  12\n",
            "Episodio  12 / 50000\n",
            "target:  [2.31898761] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.0067138671875\n",
            "\n",
            "\n",
            "current_episode =  13\n",
            "Episodio  13 / 50000\n",
            "target:  [2.31905222] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.00335693359375\n",
            "\n",
            "\n",
            "current_episode =  14\n",
            "Episodio  14 / 50000\n",
            "target:  [2.31744576] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.001678466796875\n",
            "\n",
            "\n",
            "current_episode =  15\n",
            "Episodio  15 / 50000\n",
            "target:  [2.31512642] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.0008392333984375\n",
            "\n",
            "\n",
            "current_episode =  16\n",
            "Episodio  16 / 50000\n",
            "target:  [2.31618142] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.00041961669921875\n",
            "\n",
            "\n",
            "current_episode =  17\n",
            "Episodio  17 / 50000\n",
            "target:  [2.31405926] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.000209808349609375\n",
            "\n",
            "\n",
            "current_episode =  18\n",
            "Episodio  18 / 50000\n",
            "target:  [2.31181097] \n",
            "reward:  -0.0 \n",
            "average_reward:  -0.0001049041748046875\n",
            "\n",
            "\n",
            "current_episode =  19\n",
            "Episodio  19 / 50000\n",
            "target:  [2.30651999] \n",
            "reward:  -0.0 \n",
            "average_reward:  -5.245208740234375e-05\n",
            "\n",
            "\n",
            "current_episode =  20\n",
            "Episodio  20 / 50000\n",
            "target:  [2.30740118] \n",
            "reward:  -0.0 \n",
            "average_reward:  -2.6226043701171875e-05\n",
            "\n",
            "\n",
            "current_episode =  21\n",
            "Episodio  21 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -1.3113021850585938e-05\n",
            "\n",
            "\n",
            "current_episode =  22\n",
            "Episodio  22 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -6.556510925292969e-06\n",
            "\n",
            "\n",
            "current_episode =  23\n",
            "Episodio  23 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -3.2782554626464844e-06\n",
            "\n",
            "\n",
            "current_episode =  24\n",
            "Episodio  24 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -1.6391277313232422e-06\n",
            "\n",
            "\n",
            "current_episode =  25\n",
            "Episodio  25 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -8.195638656616211e-07\n",
            "\n",
            "\n",
            "current_episode =  26\n",
            "Episodio  26 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -4.0978193283081055e-07\n",
            "\n",
            "\n",
            "current_episode =  27\n",
            "Episodio  27 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -2.0489096641540527e-07\n",
            "\n",
            "\n",
            "current_episode =  28\n",
            "Episodio  28 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -1.0244548320770264e-07\n",
            "\n",
            "\n",
            "current_episode =  29\n",
            "Episodio  29 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -5.122274160385132e-08\n",
            "\n",
            "\n",
            "current_episode =  30\n",
            "Episodio  30 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -2.561137080192566e-08\n",
            "\n",
            "\n",
            "current_episode =  31\n",
            "Episodio  31 / 50000\n",
            "target:  [nan] \n",
            "reward:  -0.0 \n",
            "average_reward:  -1.280568540096283e-08\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-aff5dba30d48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Risultato della minimizzazione\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mobjective_minimized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Nelder-Mead'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Compongo input della predizione di q'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nelder-mead'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m         res = _minimize_neldermead(fun, x0, args, callback, bounds=bounds,\n\u001b[0m\u001b[1;32m    685\u001b[0m                                    **options)\n\u001b[1;32m    686\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'powell'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_minimize_neldermead\u001b[0;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, bounds, **unknown_options)\u001b[0m\n\u001b[1;32m    905\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mbounds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                             \u001b[0mxcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m                         \u001b[0mfxcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mfxcc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfsim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(x, *wrapper_args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# A copy of x is sent to the user function (gh13740)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Ideally, we'd like to a have a true scalar returned from f(x). For\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# backwards-compatibility, also allow np.array([1.3]),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-9e4404fa430e>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mnn_input_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_s_prime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mnn_input_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_input_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mfunct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrete_neurale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_input_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfunct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2346\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2348\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2349\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2350\u001b[0m                         \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m             can_run_full_execution = (\n\u001b[1;32m   1373\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    641\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mread_value_no_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   4067\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4069\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   4070\u001b[0m         _ctx, \"Identity\", name, input)\n\u001b[1;32m   4071\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pf7gmbMTPQg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
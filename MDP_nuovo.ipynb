{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drive & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/edoardorossi/Documents/Universita/Tesi/Tesi_GDrive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 14:38:10.016622: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "IN_COLAB = False\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  pass\n",
    "\n",
    "if IN_COLAB == True:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive')\n",
    "  %cd /content/gdrive/MyDrive/Tesi_GDrive\n",
    "  PROJECT_PATH = '/content/gdrive/MyDrive/Tesi_GDrive'\n",
    "  #!ls\n",
    "else:\n",
    "  %cd /Users/edoardorossi/Documents/Universita/Tesi/Tesi_GDrive\n",
    "  PROJECT_PATH = '/Users/edoardorossi/Documents/Universita/Tesi/Tesi_GDrive'\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if IN_COLAB == True:\n",
    "  sys.path.insert(0, os.path.abspath('functions'))\n",
    "  sys.path.insert(0, os.path.abspath(''))\n",
    "else:\n",
    "  sys.path.insert(0, os.path.abspath('functions'))\n",
    "  sys.path.insert(0, os.path.abspath(''))\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.layers import Activation\n",
    "from keras.activations import exponential, relu\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.layers import Input, Layer, InputSpec, Reshape\n",
    "#from tensorflow.keras import initializers\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "tf.config.run_functions_eagerly(True)\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "\n",
    "from pkl import store_data, load_data\n",
    "#from functions.compressor import compress\n",
    "from functions.compressor_param import compress_2\n",
    "from functions.mapping import ind2sub, sub2ind3d\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 2176378 rows and 76 columns\n"
     ]
    }
   ],
   "source": [
    "cta30_def_path = os.path.join(PROJECT_PATH, 'CTA30/DF_clean/clean_CTA_30_def.csv')  \n",
    "df_def = pd.read_csv(cta30_def_path)\n",
    "\n",
    "\n",
    "cta30_18_path = os.path.join(PROJECT_PATH, 'CTA30/DF_clean/clean_CTA_30_2018.csv')\n",
    "cta30_19_path = os.path.join(PROJECT_PATH, 'CTA30/DF_clean/clean_CTA_30_2019.csv')\n",
    "cta30_20_path = os.path.join(PROJECT_PATH, 'CTA30/DF_clean/clean_CTA_30_2020.csv')\n",
    "cta30_21_path = os.path.join(PROJECT_PATH, 'CTA30/DF_clean/clean_CTA_30_2021.csv')\n",
    "cta30_22_path = os.path.join(PROJECT_PATH, 'CTA30/DF_clean/clean_CTA_30_2022.csv')\n",
    "\n",
    "df18= pd.read_csv(cta30_18_path)\n",
    "df19= pd.read_csv(cta30_19_path)\n",
    "df20= pd.read_csv(cta30_20_path)\n",
    "df21= pd.read_csv(cta30_21_path)\n",
    "df22= pd.read_csv(cta30_22_path)\n",
    "\n",
    "df = pd.concat([df18, df19, df20, df21, df22], ignore_index=True)\n",
    "\n",
    "[r,c] = df.shape\n",
    "print(\"The dataset has\", r, \"rows and\", c, \"columns\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing - \"Compression\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scale_norm = MinMaxScaler()\n",
    "df_norm = pd.DataFrame(scale_norm.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_scale = StandardScaler()\n",
    "df_st = pd.DataFrame(st_scale.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing - \"Clustering\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Means_silhouette_best_model(df, k_max):\n",
    "\n",
    "    X = df.values\n",
    "\n",
    "    n_clusters_range = range(2, k_max+1)\n",
    "\n",
    "    silhouettes_score = []\n",
    "    models = []\n",
    "\n",
    "    for n_clusters in n_clusters_range:\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "\n",
    "        silhouettes_score.append(silhouette_avg)\n",
    "        models.append(kmeans)\n",
    "    \n",
    "    best_model_idx = np.argmax(silhouettes_score)\n",
    "    best_model = models[best_model_idx]\n",
    "    best_n_clusters = n_clusters_range[best_model_idx]\n",
    "    print(\"Best model has\", best_n_clusters, \"clusters\")\n",
    "\n",
    "    best_model.fit(X)\n",
    "    labels = best_model.predict(X)\n",
    "\n",
    "    df['cluster'] = pd.Series(labels, index=df.index)\n",
    "\n",
    "    return df, best_model, best_n_clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_kmeans_return = K_Means_silhouette_best_model(df_st, 100)\n",
    "norm_kmeans_return = K_Means_silhouette_best_model(df_norm, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Medioids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
